from IPython import get_ipython
from IPython.display import display
# %%
pip install -U --upgrade tensorflow
# %%
from tensorflow import keras

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os
# %%
import os
import zipfile

DATA_FOLDER = "/content/train_sample_videos (2).zip"
TRAIN_SAMPLE_FOLDER = 'train_sample_videos'
TEST_FOLDER = 'test_videos'

# Extract the zip file to a temporary directory
with zipfile.ZipFile(DATA_FOLDER, 'r') as zip_ref:
    zip_ref.extractall('/tmp/train_sample_videos')

# Update DATA_FOLDER to point to the extracted directory
DATA_FOLDER = '/tmp/train_sample_videos'

print(f"train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}")
print(f"test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}")
# %%
DATA_FOLDER = "/content/train_sample_videos (2).zip"
TRAIN_SAMPLE_FOLDER = 'train_sample_videos'
TEST_FOLDER = 'test_videos'

print(f"train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}")
print(f"test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}")
# %%
import zipfile
import os

# Define the path of the uploaded zip file
zip_path = "/content/train_sample_videos (2).zip"  # Make sure this matches your uploaded file name
extract_folder = "/content/videos"  # Folder where files will be extracted

# Extract the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)

print("Extraction completed.")

# %%
import zipfile
import os

# Define the path to the ZIP file
zip_path = "/content/train_sample_videos.zip"
extract_path = "/content/train_videos"

# Extract the ZIP file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Check extracted files
extracted_files = os.listdir(extract_path)
print(f"Extracted files: {len(extracted_files)}")
print(extracted_files[:10])  # Print first 10 files for reference
# %%
TRAIN_SAMPLE_FOLDER = "/content/train_videos"

# Count the number of training videos
train_videos = os.listdir(TRAIN_SAMPLE_FOLDER)
print(f"Number of training videos: {len(train_videos)}")
# %%
import os

# Define the path where files are stored in Google Colab
DATA_FOLDER = '/content'  # Colab stores uploaded files in '/content'

# List all files in the directory to confirm they exist
all_files = os.listdir(DATA_FOLDER)
print(f"Extracted files: {len(all_files)}")
print(all_files[:10])  # Print first 10 file names for verification

# Count the number of videos (assuming they are MP4 files)
train_videos = [file for file in all_files if file.endswith('.mp4')]
print(f"Number of training videos: {len(train_videos)}")

# %%
import pandas as pd

# Correct path in Google Colab
metadata_path = "/content/videos/metadata.json"  # Adjust this if metadata.json is in a different location

# Load the metadata file
train_sample_metadata = pd.read_json(metadata_path).T
train_sample_metadata.head()

# %%
import zipfile

DATA_FOLDER = '/content'
ZIP_FILE = os.path.join(DATA_FOLDER, 'train_sample_videos.zip')

# Extract ZIP file
with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:
    zip_ref.extractall(DATA_FOLDER)

print("Extraction complete. Listing extracted files:")
print(os.listdir(DATA_FOLDER))  # Check what was extracted


# %%
import os

# List extracted files
extracted_files = os.listdir(DATA_FOLDER)
print("Extracted files:", extracted_files)
# %%
import pandas as pd

metadata_path = os.path.join(DATA_FOLDER, 'metadata.json')  # Adjust path if needed
train_sample_metadata = pd.read_json(metadata_path).T

print(train_sample_metadata.head())  # Check if data is loaded correctly
# %%
train_sample_metadata.groupby('label')['label'].count().plot(figsize=(5,5),kind='bar',title='The Label in the Training Set')
plt.show()
# %%
train_sample_metadata.shape
# %%
f_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].sample(5).index)
f_train_sample_video
# %%
def capture_image_from_video(video_path):
    # Capture video
    capture_image = cv2.VideoCapture(video_path)
    ret, frame = capture_image.read()

    if not ret:
        print(f"Could not read frame from {video_path}")
        return

    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    fig = plt.figure(figsize=(4, 4))
    ax = fig.add_subplot(111)
    ax.imshow(frame)
    plt.show()
    capture_image.release()
# %%
for video_file in f_train_sample_video:
    capture_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))
# %%
r_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='REAL'].sample(5).index)
r_train_sample_video
# %%
for video_file in r_train_sample_video:
    capture_image_from_video(os.path.join(DATA_FOLDER,TRAIN_SAMPLE_FOLDER,video_file))
# %%
f_videos = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].index)
from IPython.display import HTML
from base64 import b64encode

def play_video(video_file,subset=TRAIN_SAMPLE_FOLDER):
    video_url = open(os.path.join(DATA_FOLDER,subset,video_file),'rb').read()
    data_url = "data:video/mp4;base64," + b64encode(video_url).decode()
    return HTML("""<video width=500 controls><source src="%s" type="video/mp4"></video>""" %data_url)
# %%
img_size = 224
batch_size = 64
epochs = 15

max_seq_length = 20
num_features = 2048
# %%
def crop_center_square(frame):
    y,x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y :start_y + min_dim, start_x : start_x + min_dim]

def load_video(path, max_frames=0, resize=(img_size, img_size)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while 1:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]
            frames.append(frame)

            if len(frames) == max_frames:
                break
    finally:
        cap.release()
    return np.array(frames)
# %%
def pretrain_feature_extractor():
    feature_extractor = keras.applications.InceptionV3(
    weights = "imagenet",
    include_top=False,
    pooling="avg",
    input_shape = (img_size,img_size,3)
    )
    preprocess_input = keras.applications.inception_v3.preprocess_input

    inputs = keras.Input((img_size,img_size,3))
    preprocessed = preprocess_input(inputs)

    outputs = feature_extractor(preprocessed)
    return keras.Model(inputs, outputs, name="feature_extractor")

feature_extractor = pretrain_feature_extractor()
# %%
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = list(df.index)
    labels = df["label"].values
    labels = np.array(labels=='FAKE').astype(np.int)

    frame_masks = np.zeros(shape=(num_samples, max_seq_length), dtype="bool")
    frame_features = np.zeros(
        shape=(num_samples, max_seq_length, num_features), dtype="float32"
    )

    for idx, path in enumerate(video_paths):
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        temp_frame_mask = np.zeros(shape=(1, max_seq_length,), dtype="bool")
        temp_frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype="float32")

        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(max_seq_length, video_length) #if length is over 20s ,only cut 20s
            for j in range(length):
                temp_frame_features[i, j, :] =feature_extractor.predict(batch[None, j, :])
            temp_frame_mask[i, :length] =1 # 1 = not masked, 0 = masked ->give 1 when there are images ,otherwise 0 for padding

        frame_features[idx,] =temp_frame_features.squeeze() #squeeze array for training
        frame_masks[idx,] =temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels
# %%
from sklearn.model_selection import train_test_split

Train_set , Test_set = train_test_split(train_sample_metadata, test_size=0.3,random_state=42,
                                       stratify=train_sample_metadata['label'])
print(Train_set.shape, Test_set.shape)
# %%
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = list(df.index)
    labels = df["label"].values
    labels = np.array(labels=='FAKE').astype(int) # Changed np.int to int

    frame_masks = np.zeros(shape=(num_samples, max_seq_length), dtype="bool")
    frame_features = np.zeros(
        shape=(num_samples, max_seq_length, num_features), dtype="float32"
    )

    for idx, path in enumerate(video_paths):
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        temp_frame_mask = np.zeros(shape=(1, max_seq_length,), dtype="bool")
        temp_frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype="float32")

        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(max_seq_length, video_length) #if length is over 20s ,only cut 20s
            for j in range(length):
                temp_frame_features[i, j, :] =feature_extractor.predict(batch[None, j, :])
            temp_frame_mask[i, :length] =1 # 1 = not masked, 0 = masked ->give 1 when there are images ,otherwise 0 for padding

        frame_features[idx,] =temp_frame_features.squeeze() #squeeze array for training
        frame_masks[idx,] =temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels
# %%
frame_features_input = keras.Input((max_seq_length, num_features))
mask_input = keras.Input((max_seq_length,),dtype="bool")

x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask = mask_input)
x = keras.layers.GRU(8)(x)
x = keras.layers.Dropout(0.4)(x)
x = keras.layers.Dense(8, activation="relu")(x)
output = keras.layers.Dense(1, activation="sigmoid")(x)

model = keras.Model([frame_features_input, mask_input], output)
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
model.summary()
# %%
import os
import cv2
import numpy as np
import pandas as pd
from tensorflow import keras
from sklearn.model_selection import train_test_split
import joblib  # For caching features

# ... (your other functions, including load_video, pretrain_feature_extractor, etc.) ...

def extract_features_and_cache(df, root_dir, cache_dir='feature_cache'):
    """Extracts features and caches them to disk."""
    os.makedirs(cache_dir, exist_ok=True)

    for video_path in df.index:
        cache_path = os.path.join(cache_dir, video_path.replace('/', '_') + '.pkl')

        if not os.path.exists(cache_path):  # Only extract if not cached
            frames = load_video(os.path.join(root_dir, video_path))
            features = []
            for frame in frames:
                features.append(feature_extractor.predict(frame[None, ...]))
            features = np.array(features)
            joblib.dump(features, cache_path)  # Save features to cache
            print(f"Features extracted and cached for: {video_path}")
        else:
            print(f"Features loaded from cache for: {video_path}")

def data_generator(df, root_dir, cache_dir='feature_cache', batch_size=32):
    """Generates batches of data for training."""
    while True:
        # Shuffle the DataFrame
        df = df.sample(frac=1).reset_index(drop=True)

        num_samples = len(df)
        num_batches = num_samples // batch_size

        for i in range(num_batches):
            batch_df = df[i * batch_size : (i + 1) * batch_size]
            frame_features, frame_masks, labels = prepare_batch(batch_df, root_dir, cache_dir)
            yield ([frame_features, frame_masks], labels)

def prepare_batch(df, root_dir, cache_dir='feature_cache'):
    """Prepares a single batch of data."""
    num_samples = len(df)
    video_paths = df.index
    labels = df["label"].values
    labels = np.array(labels == 'FAKE').astype(int)

    frame_masks = np.zeros(shape=(num_samples, max_seq_length), dtype="bool")
    frame_features = np.zeros(shape=(num_samples, max_seq_length, num_features), dtype="float32")

    for idx, path in enumerate(video_paths):
        cache_path = os.path.join(cache_dir, path.replace('/', '_') + '.pkl')
        features = joblib.load(cache_path)  # Load features from cache

        video_length = features.shape[0]
        length = min(max_seq_length, video_length)
        frame_features[idx, :length, :] = features[:length]
        frame_masks[idx, :length] = 1

    return frame_features, frame_masks, labels

# ... (Your model definition) ...

# 1. Extract and cache features:
extract_features_and_cache(train_sample_metadata, DATA_FOLDER)

# 2. Perform train-test split:
Train_set, Test_set = train_test_split(train_sample_metadata, test_size=0.3, random_state=42, stratify=train_sample_metadata['label'])

# 3. Create data generators:
train_gen = data_generator(Train_set, DATA_FOLDER, batch_size=batch_size)
test_gen = data_generator(Test_set, DATA_FOLDER, batch_size=batch_size)

# 4. Train the model using the generators:
history = model.fit(
    train_gen,
    steps_per_epoch=len(Train_set) // batch_size,  # Number of batches per epoch
    validation_data=test_gen,
    validation_steps=len(Test_set) // batch_size,  # Number of validation batches
    epochs=epochs,
)
# %%
import os
print(os.listdir("/kaggle/working"))
# %%
test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])
def prepare_single_video(frames):
    frames = frames[None, ...]
    frame_mask = np.zeros(shape=(1, max_seq_length,), dtype="bool")
    frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype="float32")

    for i, batch in enumerate(frames):
        video_length = batch.shape[0]
        length = min(max_seq_length, video_length)
        for j in range(length):
            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])
        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

    return frame_features, frame_mask

def sequence_prediction(path):
    frames = load_video(os.path.join(DATA_FOLDER, TEST_FOLDER,path))
    frame_features, frame_mask = prepare_single_video(frames)
    return model.predict([frame_features, frame_mask])[0]

# This utility is for visualization.
# Referenced from:
# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub
def to_gif(images):
    converted_images = images.astype(np.uint8)
    imageio.mimsave("animation.gif", converted_images, fps=10)
    return embed.embed_file("animation.gif")


test_video = np.random.choice(test_videos["video"].values.tolist())
print(f"Test video path: {test_video}")

if(sequence_prediction(test_video)>=0.5):
    print(f'The predicted class of the video is FAKE')
else:
    print(f'The predicted class of the video is REAL')

play_video(test_video,TEST_FOLDER)
# %%
model.save("/kaggle/working/fake_video_detection_model.h5")
# %%
import os
print(os.listdir("/kaggle/working"))
# %%
test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])
def prepare_single_video(frames):
    frames = frames[None, ...]
    frame_mask = np.zeros(shape=(1, max_seq_length,), dtype="bool")
    frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype="float32")

    for i, batch in enumerate(frames):
        video_length = batch.shape[0]
        length = min(max_seq_length, video_length)
        for j in range(length):
            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])
        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

    return frame_features, frame_mask

def sequence_prediction(path):
    frames = load_video(os.path.join(DATA_FOLDER, TEST_FOLDER,path))
    frame_features, frame_mask = prepare_single_video(frames)
    return model.predict([frame_features, frame_mask])[0]

# This utility is for visualization.
# Referenced from:
# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub
def to_gif(images):
    converted_images = images.astype(np.uint8)
    imageio.mimsave("animation.gif", converted_images, fps=10)
    return embed.embed_file("animation.gif")


test_video = np.random.choice(test_videos["video"].values.tolist())
print(f"Test video path: {test_video}")

if(sequence_prediction(test_video)>=0.5):
    print(f'The predicted class of the video is FAKE')
else:
    print(f'The predicted class of the video is REAL')

play_video(test_video,TEST_FOLDER)
# %%

# %%
import numpy as np
import os
import cv2
import tensorflow as tf
from tensorflow import keras
from IPython.display import HTML
import base64

# Load the trained model
model = keras.models.load_model("/kaggle/working/fake_video_detection_model.h5")

# Define constants
max_seq_length = 20  # Should be the same as in training
num_features = 2048   # Adjust according to feature extractor

# Load the feature extractor (same one used in training)
feature_extractor = keras.applications.InceptionV3(include_top=False, pooling="avg", input_shape=(224, 224, 3))

def load_video(video_path, max_frames=max_seq_length, frame_size=(224, 224)):
    """Loads video frames and resizes them to the required shape."""
    cap = cv2.VideoCapture(video_path)
    frames = []
    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret or frame_count >= max_frames:
            break
        frame = cv2.resize(frame, frame_size)  # Resize to match feature extractor
        frame = frame / 255.0  # Normalize
        frames.append(frame)
        frame_count += 1

    cap.release()

    if len(frames) < max_frames:
        # Pad with zeros if less frames
        frames += [np.zeros((224, 224, 3))] * (max
